{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjFRIAZE7644"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Data Augmentation for trainig(I added a reference to the github link I got it from)\n",
    "train_transform = transforms.Compose([\n",
    "   transforms.RandomHorizontalFlip(),\n",
    "   transforms.RandomCrop(32, padding=4),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Normalisation for Test\n",
    "test_transform = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "\n",
    "#Dataset Loaders\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=train_transform)\n",
    "trainIter = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=test_transform)\n",
    "testIter = DataLoader(testset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# First layer processes the image and extracts the features\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Stem, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, num_outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "# generate weights\n",
    "class Expert_branch(nn.Module):\n",
    "    def __init__(self, input, r, k):\n",
    "        super(Expert_branch, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(input, input // r)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input // r, k)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.avgpool(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out=self.fc1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "#c onv branch with k convs\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, input, output, k):\n",
    "        super(ConvBranch, self).__init__()\n",
    "        self.k = k\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(k):\n",
    "            self.convs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input,output,kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(output)\n",
    "                )\n",
    "            )\n",
    "    def forward(self, x, a):\n",
    "        out = 0\n",
    "        for i in range(self.k):\n",
    "            weight = a[:, i].view(-1, 1, 1, 1) # help from github\n",
    "            out += weight * self.convs[i](x)\n",
    "        return out\n",
    "\n",
    "# combines expert + conv and adds skip connection(like ResNet from the lectures)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, input, output, k, r):\n",
    "        super().__init__()\n",
    "        self.expert = Expert_branch(input, r, k)\n",
    "        self.conv = ConvBranch(input, output, k)\n",
    "        self.relu = nn.ReLU()\n",
    "        if input != output:\n",
    "            self.skip = nn.Conv2d(input, output, kernel_size=1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.expert(x)\n",
    "        out = self.conv(x, a)\n",
    "        out += self.skip(x) #residual connection\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# classifier with global pooling,mlp and dropouts\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(in_channels,1024)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.drop1=nn.Dropout(0.4)\n",
    "        self.fc2=nn.Linear(1024,512)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.drop2=nn.Dropout(0.3)\n",
    "        self.fc3=nn.Linear(512,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.drop1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.drop2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model definition\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10Model, self).__init__()\n",
    "        self.stem = Stem(32)\n",
    "        self.block1 = Block(32, 64, k=4, r=8)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.block2 = Block(64, 128, k=6, r=4)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.block3 = Block(128, 256, k=6, r=4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.block4 = Block(256, 512, k=6, r=4)\n",
    "\n",
    "        self.classifier = Classifier(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        x= self.pool3(x)\n",
    "        x= self.block4(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CIFAR10Model().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# weight initialisation\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# training loop\n",
    "\n",
    "def trainModel(model, trainData, testData, numEpochs, lr):\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
    "    lrScheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=100)\n",
    "\n",
    "    # lists to keep track of how things are going\n",
    "    trainLossHistory = []\n",
    "    testLossHistory = []\n",
    "    trainAccuracy = []\n",
    "    testAccuracy = []\n",
    "\n",
    "    for currentEpoch in range(numEpochs):\n",
    "        model.train()\n",
    "        runningLoss = 0\n",
    "        correctPredictions = 0\n",
    "        totalExamples = 0\n",
    "        for images, labels in trainData:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optim.zero_grad()\n",
    "            predictions = model(images)\n",
    "            batchLoss = lossFunction(predictions, labels)\n",
    "            batchLoss.backward()\n",
    "            optim.step()\n",
    "            runningLoss += batchLoss.item() * labels.size(0)\n",
    "            correctPredictions += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            totalExamples += labels.size(0)\n",
    "\n",
    "        avgTrainLoss = runningLoss / totalExamples\n",
    "        trainAcc = correctPredictions / totalExamples\n",
    "        trainLossHistory.append(avgTrainLoss)\n",
    "        trainAccuracy.append(trainAcc)\n",
    "        model.eval()\n",
    "        testLoss = 0\n",
    "        testCorrect = 0\n",
    "        testTotal = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for testImages, testLabels in testData:\n",
    "                testImages = testImages.to(device)\n",
    "                testLabels = testLabels.to(device)\n",
    "\n",
    "                testOutputs = model(testImages)\n",
    "                loss = lossFunction(testOutputs, testLabels)\n",
    "\n",
    "                testLoss += loss.item() * testLabels.size(0)\n",
    "                testCorrect += (testOutputs.argmax(dim=1) == testLabels).sum().item()\n",
    "                testTotal += testLabels.size(0)\n",
    "\n",
    "        avgTestLoss = testLoss / testTotal\n",
    "        testAcc = testCorrect / testTotal\n",
    "        testLossHistory.append(avgTestLoss)\n",
    "        testAccuracy.append(testAcc)\n",
    "\n",
    "\n",
    "        lrScheduler.step()\n",
    "\n",
    "        print(f\"Epoch {currentEpoch+1}/{numEpochs} | train loss: {avgTrainLoss:.3f}, test loss: {avgTestLoss:.3f} | train acc: {trainAcc:.3f}, test acc: {testAcc:.3f}\")\n",
    "\n",
    "\n",
    "    return trainLossHistory, testLossHistory, trainAccuracy, testAccuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "train_losses, test_losses,train_accs, test_accs = trainModel(model, trainIter, testIter,\n",
    "                                                  numEpochs=num_epochs, lr=learning_rate)\n",
    "\n",
    "# Plot training and test losses\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss over epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and test accuracies\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs+1), test_accs, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Test Loss over epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
